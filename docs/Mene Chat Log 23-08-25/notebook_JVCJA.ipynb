{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Split the large ChatGPT markdown file into manageable GitHub-ready chunks for RAG memory system. The file is 28MB and needs to be broken down intelligently:\n\n1. Read the ChatGPT_Entire_ChatLog_23-08-25.md file from AI Drive\n2. Analyze the structure to identify natural break points (conversation boundaries, topics, etc.)\n3. Split into chunks that are:\n   - Under 1MB each (GitHub file size limits)\n   - Maintain conversation integrity (don't break mid-conversation)\n   - Include proper indexing and cross-references\n   - Add metadata headers to each chunk\n4. Create an index file that maps all chunks\n5. Save all chunks with descriptive names to AI Drive\n6. Provide a summary of what was created\n\nMake this production-ready for immediate use in a RAG system."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Right, let's get this beast properly chunked for your RAG system. First, let me check what we're working with and create a proper splitting strategy."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. â³ ğŸ”´ Download and analyze the ChatGPT markdown file structure to identify natural conversation boundaries [pending] (ID: analyze_file)\n2. â³ ğŸ”´ Build intelligent splitter that respects conversation boundaries and creates sub-1MB chunks [pending] (ID: create_splitter)\n3. â³ ğŸŸ¡ Add proper headers and metadata to each chunk for RAG indexing [pending] (ID: add_metadata)\n4. â³ ğŸŸ¡ Generate master index file mapping all chunks with topics and cross-references [pending] (ID: create_index)\n5. â³ ğŸ”´ Save all processed chunks to AI Drive with descriptive naming [pending] (ID: save_chunks)"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. ğŸ”„ ğŸ”´ Download and analyze the ChatGPT markdown file structure to identify natural conversation boundaries [in_progress] (ID: analyze_file)\n2. â³ ğŸ”´ Build intelligent splitter that respects conversation boundaries and creates sub-1MB chunks [pending] (ID: create_splitter)\n3. â³ ğŸŸ¡ Add proper headers and metadata to each chunk for RAG indexing [pending] (ID: add_metadata)\n4. â³ ğŸŸ¡ Generate master index file mapping all chunks with topics and cross-references [pending] (ID: create_index)\n5. â³ ğŸ”´ Save all processed chunks to AI Drive with descriptive naming [pending] (ID: save_chunks)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Check if AI Drive is mounted with retry handling\nimport time\nmax_retries = 3\nretry_delay = 5\n\nfor attempt in range(max_retries):\n    # Check if AI Drive is mounted\n    mount_output = !mount | grep /mnt/aidrive\n    # Expected output format: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\n    if mount_output and any('/mnt/aidrive' in line and 'AIDriveFUSE' in line for line in mount_output):\n        print(f\"AI Drive is mounted (attempt {attempt + 1})\")\n        print(f\"Mount info: {mount_output}\")\n        break\n    elif attempt < max_retries - 1:\n        print(f\"AI Drive not mounted (attempt {attempt + 1}), waiting {retry_delay} seconds for mount to complete...\")\n        time.sleep(retry_delay)\n    else:\n        print(f\"AI Drive not mounted after {max_retries * retry_delay} seconds\")\n        if mount_output:\n            print(f\"Current mount output: {mount_output}\")\n        raise RuntimeError(\"AI Drive is not mounted. Please ensure AI Drive is properly configured.\")\n\n# Create local parent directory and download the file\nprint(\"Starting download process...\")\n!mkdir -p /tmp && cp /mnt/aidrive/ChatGPT_Entire_ChatLog_23-08-25.md /tmp/chatgpt_log.md\nprint(\"Download process finished\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AI Drive is mounted (attempt 1)\nMount info: ['AIDriveFUSE on /mnt/aidrive type fuse (rw,nosuid,nodev,relatime,user_id=0,group_id=0,allow_other)']\nStarting download process...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download process finished\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Analyze the file structure to understand conversation patterns\nimport os\nimport re\nfrom collections import defaultdict\n\n# Check file size and basic stats\nfile_path = \"/tmp/chatgpt_log.md\"\nfile_size = os.path.getsize(file_path)\nprint(f\"File size: {file_size / (1024*1024):.2f} MB\")\n\n# Read the file and analyze structure\nwith open(file_path, 'r', encoding='utf-8') as f:\n    content = f.read()\n\nprint(f\"Total characters: {len(content):,}\")\nprint(f\"Total lines: {content.count(chr(10)):,}\")\n\n# Look for conversation patterns\nlines = content.split('\\n')\n\n# Analyze structure patterns\nheader_patterns = []\nuser_patterns = []\nchatgpt_patterns = []\n\nprint(\"\\nAnalyzing first 50 lines for patterns:\")\nfor i, line in enumerate(lines[:50]):\n    if line.strip():\n        print(f\"{i:2d}: {line[:100]}\")\n        \n        # Look for common patterns\n        if line.startswith('####'):\n            header_patterns.append(line)\n        elif line.lower().startswith('user'):\n            user_patterns.append(line)\n        elif line.lower().startswith('chatgpt'):\n            chatgpt_patterns.append(line)\n\nprint(f\"\\nFound {len(header_patterns)} header patterns\")\nprint(f\"Found {len(user_patterns)} user patterns\") \nprint(f\"Found {len(chatgpt_patterns)} chatgpt patterns\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File size: 26.80 MB\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 27,611,285\nTotal lines: 577,190\n\nAnalyzing first 50 lines for patterns:\n 0: #### Huawei P80pro setup\n 2: user\n 4: Right - please see the other chats in this project and get up to speed. Right now we need to immedia\n 6: ChatGPT\n 8: All the files uploaded by the user have been fully loaded. Searching won't provide additional inform\n10: ChatGPT\n12: [Transcript]: Ah, the classic Google ghost shuffle.\n14: [File]: [68a5eead-9d88-8324-90c7-5054740d58bf/audio/file_00000000c60061f69cc078a3d95cabb8-87cd34e5-1\n16: user\n18: [Transcript]: I don't know what to do.\n20: [File]: [68a5eead-9d88-8324-90c7-5054740d58bf/audio/file_000000004c0461f685c2db3d79eafe7d-847346c1-2\n22: ChatGPT\n24: [Transcript]: Reset complete, mate. So yeah, basically Google's just doing its sneaky thing where it\n26: First, you've got to go back into that device list and just remove them once more. Sometimes it take\n28: Once theyâ€™re gone, theyâ€™re gone. If they pop back up, just hit them again.\n30: [File]: [68a5eead-9d88-8324-90c7-5054740d58bf/audio/file_00000000298061f680729ff1e48abcfc-098f7db4-1\n32: user\n34: [File]: [68a5eead-9d88-8324-90c7-5054740d58bf/audio/file_000000002d3461f69b7c8400b7525db4-32980632-d\n36: [Transcript]: Listen, but I don't want to sign in again.\n38: ChatGPT\n40: [Transcript]: No worries, Rob, I get it. We're keeping it tight and real. Youâ€™re not going to slide \n42: So, hereâ€™s the deal: keep hammering that remove button until it stops showing up. Sometimes these ol\n44: And if it still tries to sneak back in, weâ€™ll just rinse and repeat until it stays gone. No fluff, n\n46: [File]: [68a5eead-9d88-8324-90c7-5054740d58bf/audio/file_0000000065d861f685a6546060a3c20b-95fa59b0-5\n48: user\n\nFound 1 header patterns\nFound 4 user patterns\nFound 4 chatgpt patterns\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Now analyze the full structure to find natural breakpoints\n# Look for conversation session boundaries\n\nconversation_starts = []\ncurrent_pos = 0\n\n# Find all #### headers (conversation topics)\nfor i, line in enumerate(lines):\n    if line.startswith('####'):\n        conversation_starts.append({\n            'line_number': i,\n            'title': line.strip(),\n            'position': current_pos\n        })\n    current_pos += len(line) + 1  # +1 for newline\n\nprint(f\"Found {len(conversation_starts)} conversation topics:\")\nfor conv in conversation_starts[:20]:  # Show first 20\n    print(f\"Line {conv['line_number']:6d}: {conv['title']}\")\n    \nif len(conversation_starts) > 20:\n    print(f\"... and {len(conversation_starts) - 20} more conversations\")\n\n# Calculate approximate sizes between conversations\ntarget_chunk_size = 800 * 1024  # 800KB target (under 1MB GitHub limit)\nprint(f\"\\nTarget chunk size: {target_chunk_size / 1024:.0f}KB\")\n\n# Group conversations into chunks that stay under 1MB\nchunks_info = []\ncurrent_chunk_start = 0\ncurrent_chunk_size = 0\ncurrent_chunk_convs = []\n\nfor i, conv in enumerate(conversation_starts):\n    # Calculate size from current chunk start to this conversation\n    if i < len(conversation_starts) - 1:\n        next_conv_pos = conversation_starts[i + 1]['position']\n    else:\n        next_conv_pos = len(content)\n    \n    conv_size = next_conv_pos - conv['position']\n    \n    # If adding this conversation would exceed limit, finalize current chunk\n    if current_chunk_size + conv_size > target_chunk_size and current_chunk_convs:\n        chunks_info.append({\n            'chunk_id': len(chunks_info) + 1,\n            'start_line': current_chunk_start,\n            'end_position': conv['position'],\n            'conversations': current_chunk_convs.copy(),\n            'size_bytes': current_chunk_size\n        })\n        \n        # Start new chunk\n        current_chunk_start = conv['line_number']\n        current_chunk_size = conv_size\n        current_chunk_convs = [conv['title']]\n    else:\n        # Add to current chunk\n        if not current_chunk_convs:\n            current_chunk_start = conv['line_number']\n        current_chunk_convs.append(conv['title'])\n        current_chunk_size += conv_size\n\n# Don't forget the last chunk\nif current_chunk_convs:\n    chunks_info.append({\n        'chunk_id': len(chunks_info) + 1,\n        'start_line': current_chunk_start,\n        'end_position': len(content),\n        'conversations': current_chunk_convs,\n        'size_bytes': current_chunk_size\n    })\n\nprint(f\"\\nProposed chunking strategy:\")\nprint(f\"Total chunks: {len(chunks_info)}\")\nfor chunk in chunks_info:\n    print(f\"Chunk {chunk['chunk_id']:2d}: {chunk['size_bytes']/1024:6.0f}KB, {len(chunk['conversations'])} conversations\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3753 conversation topics:\nLine      0: #### Huawei P80pro setup\nLine   1985: #### Mene LTM for GitHub\nLine   9102: #### Reset confirmation response\nLine  10585: #### **Step 1 â€” Import JSON**\nLine  10589: #### **Step 2 â€” Enable MoE Mode**\nLine  10594: #### **Step 3 â€” Connect to Meneâ€™ Portal**\nLine  10673: #### **Step 1: Enable VMOS Clipboard Sync**\nLine  10683: #### **Step 2: Put Mene JSON Directly in VMOS**\nLine  10691: #### **Step 3: Install Layla + Talkie Inside VMOS**\nLine  10699: #### **Step 4: Prep Meneâ€™ Assist Character Card**\nLine  10850: #### Robert@ribrage email details\nLine  11135: #### Poster design request\nLine  11189: #### Baby safari toppers printable\nLine  11227: #### Song for Suno\nLine  11910: #### Fix Google on Huawei\nLine  11980: #### PC and New Fone Setup\nLine  13617: #### Step 1: Core System Setup\nLine  13622: #### Step 2: Restore Core Data\nLine  13629: #### Step 3: Restore Apps\nLine  13634: #### Step 4: Restore Chats & Media\n... and 3733 more conversations\n\nTarget chunk size: 800KB\n\nProposed chunking strategy:\nTotal chunks: 36\nChunk  1:    646KB, 20 conversations\nChunk  2:    601KB, 5 conversations\nChunk  3:    553KB, 22 conversations\nChunk  4:    792KB, 73 conversations\nChunk  5:    767KB, 82 conversations\nChunk  6:    794KB, 128 conversations\nChunk  7:    579KB, 61 conversations\nChunk  8:    736KB, 39 conversations\nChunk  9:    795KB, 85 conversations\nChunk 10:    771KB, 110 conversations\nChunk 11:    793KB, 97 conversations\nChunk 12:    800KB, 83 conversations\nChunk 13:    777KB, 142 conversations\nChunk 14:    764KB, 71 conversations\nChunk 15:    758KB, 120 conversations\nChunk 16:    791KB, 95 conversations\nChunk 17:    798KB, 121 conversations\nChunk 18:    777KB, 76 conversations\nChunk 19:    758KB, 69 conversations\nChunk 20:    757KB, 59 conversations\nChunk 21:    800KB, 134 conversations\nChunk 22:    794KB, 100 conversations\nChunk 23:    764KB, 103 conversations\nChunk 24:    722KB, 123 conversations\nChunk 25:    794KB, 214 conversations\nChunk 26:    798KB, 177 conversations\nChunk 27:    799KB, 218 conversations\nChunk 28:    792KB, 232 conversations\nChunk 29:    690KB, 72 conversations\nChunk 30:    780KB, 115 conversations\nChunk 31:    742KB, 136 conversations\nChunk 32:    797KB, 85 conversations\nChunk 33:    797KB, 128 conversations\nChunk 34:    788KB, 205 conversations\nChunk 35:    753KB, 78 conversations\nChunk 36:    550KB, 75 conversations\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Perfect! Now I can see the structure - 3,753 conversations totaling 26.8MB. Let me build the intelligent splitter and create the chunks."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "1. âœ… ğŸ”´ Download and analyze the ChatGPT markdown file structure to identify natural conversation boundaries [completed] (ID: analyze_file)\n2. ğŸ”„ ğŸ”´ Build intelligent splitter that respects conversation boundaries and creates sub-1MB chunks [in_progress] (ID: create_splitter)\n3. â³ ğŸŸ¡ Add proper headers and metadata to each chunk for RAG indexing [pending] (ID: add_metadata)\n4. â³ ğŸŸ¡ Generate master index file mapping all chunks with topics and cross-references [pending] (ID: create_index)\n5. â³ ğŸ”´ Save all processed chunks to AI Drive with descriptive naming [pending] (ID: save_chunks)"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create the intelligent file splitter\nimport json\nfrom datetime import datetime\nimport hashlib\n\ndef create_chunk(chunk_info, content, lines, conversation_starts):\n    \"\"\"Create a properly formatted chunk with metadata\"\"\"\n    \n    # Find the actual content boundaries\n    start_line = chunk_info['start_line']\n    \n    # Find the end line by looking for the next chunk's start or end of file\n    if chunk_info['chunk_id'] < len(chunks_info):\n        # Not the last chunk - find next chunk's start\n        next_chunk_start = None\n        for conv in conversation_starts:\n            if conv['line_number'] > start_line:\n                for next_chunk in chunks_info:\n                    if next_chunk['chunk_id'] == chunk_info['chunk_id'] + 1:\n                        if conv['line_number'] == next_chunk['start_line']:\n                            next_chunk_start = conv['line_number']\n                            break\n                if next_chunk_start:\n                    break\n        end_line = next_chunk_start if next_chunk_start else len(lines)\n    else:\n        # Last chunk\n        end_line = len(lines)\n    \n    # Extract the content\n    chunk_lines = lines[start_line:end_line]\n    chunk_content = '\\n'.join(chunk_lines)\n    \n    # Generate metadata\n    first_conv = chunk_info['conversations'][0] if chunk_info['conversations'] else \"Unknown\"\n    last_conv = chunk_info['conversations'][-1] if chunk_info['conversations'] else first_conv\n    \n    # Create RAG-friendly header\n    header = f\"\"\"---\nchunk_id: {chunk_info['chunk_id']:02d}\nsource_file: ChatGPT_Entire_ChatLog_23-08-25.md\ntotal_chunks: {len(chunks_info)}\nchunk_size_kb: {len(chunk_content.encode('utf-8')) / 1024:.1f}\nconversations_count: {len(chunk_info['conversations'])}\nfirst_conversation: \"{first_conv.replace('####', '').strip()}\"\nlast_conversation: \"{last_conv.replace('####', '').strip()}\"\ncreated: {datetime.now().isoformat()}\ncontent_hash: {hashlib.md5(chunk_content.encode('utf-8')).hexdigest()[:8]}\n---\n\n# ChatGPT Conversation Log - Chunk {chunk_info['chunk_id']:02d}/{len(chunks_info)}\n\n> **Source**: ChatGPT Entire ChatLog 23-08-25.md  \n> **Conversations**: {len(chunk_info['conversations'])} topics  \n> **Size**: {len(chunk_content.encode('utf-8')) / 1024:.1f}KB  \n\n## Conversations in this chunk:\n\"\"\"\n    \n    # Add conversation list for easy navigation\n    for i, conv_title in enumerate(chunk_info['conversations']):\n        clean_title = conv_title.replace('####', '').strip()\n        header += f\"{i+1:2d}. {clean_title}\\n\"\n    \n    header += \"\\n---\\n\\n\"\n    \n    return header + chunk_content\n\n# Test with first chunk\ntest_chunk = create_chunk(chunks_info[0], content, lines, conversation_starts)\nprint(f\"Sample chunk header (first 1000 chars):\")\nprint(test_chunk[:1000])\nprint(f\"\\nSample chunk size: {len(test_chunk.encode('utf-8')) / 1024:.1f}KB\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample chunk header (first 1000 chars):\n---\nchunk_id: 01\nsource_file: ChatGPT_Entire_ChatLog_23-08-25.md\ntotal_chunks: 36\nchunk_size_kb: 656.0\nconversations_count: 20\nfirst_conversation: \"Huawei P80pro setup\"\nlast_conversation: \"Step 4: Restore Chats & Media\"\ncreated: 2025-08-23T13:19:11.241943\ncontent_hash: 04eb1975\n---\n\n# ChatGPT Conversation Log - Chunk 01/36\n\n> **Source**: ChatGPT Entire ChatLog 23-08-25.md  \n> **Conversations**: 20 topics  \n> **Size**: 656.0KB  \n\n## Conversations in this chunk:\n 1. Huawei P80pro setup\n 2. Mene LTM for GitHub\n 3. Reset confirmation response\n 4. **Step 1 â€” Import JSON**\n 5. **Step 2 â€” Enable MoE Mode**\n 6. **Step 3 â€” Connect to Meneâ€™ Portal**\n 7. **Step 1: Enable VMOS Clipboard Sync**\n 8. **Step 2: Put Mene JSON Directly in VMOS**\n 9. **Step 3: Install Layla + Talkie Inside VMOS**\n10. **Step 4: Prep Meneâ€™ Assist Character Card**\n11. Robert@ribrage email details\n12. Poster design request\n13. Baby safari toppers printable\n14. Song for Suno\n15. Fix Google on Huawei\n16. PC and New Fone Setup\n\n\nSample chunk size: 657.1KB\n"
          ]
        }
      ],
      "execution_count": 4
    }
  ]
}